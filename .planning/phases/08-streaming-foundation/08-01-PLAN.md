---
phase: 08-streaming-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - package-lock.json
  - src/app/api/chat/route.ts
  - src/lib/chat/constants.ts
autonomous: true

must_haves:
  truths:
    - "POST /api/chat returns streaming response"
    - "Aborting request stops generation server-side"
  artifacts:
    - path: "src/app/api/chat/route.ts"
      provides: "Streaming chat endpoint"
      exports: ["POST"]
    - path: "src/lib/chat/constants.ts"
      provides: "Chat configuration"
      exports: ["SYSTEM_PROMPT", "CHAT_MODEL"]
  key_links:
    - from: "src/app/api/chat/route.ts"
      to: "@ai-sdk/google"
      via: "createGoogleGenerativeAI import"
      pattern: "createGoogleGenerativeAI"
    - from: "src/app/api/chat/route.ts"
      to: "process.env.GEMINI_API_KEY"
      via: "API key configuration"
      pattern: "GEMINI_API_KEY"
---

<objective>
Establish streaming chat infrastructure with Vercel AI SDK and Gemini.

Purpose: Create the backend foundation for real-time AI chat responses. This enables the frontend to display responses as they stream from Gemini rather than waiting for complete responses.

Output: Working `/api/chat` endpoint that accepts messages and returns streaming responses, plus shared chat configuration constants.
</objective>

<execution_context>
@C:\Users\mkdol\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\mkdol\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-streaming-foundation/08-CONTEXT.md
@.planning/phases/08-streaming-foundation/08-RESEARCH.md
@package.json
@src/app/globals.css
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Vercel AI SDK and markdown dependencies</name>
  <files>package.json, package-lock.json</files>
  <action>
Install the required packages for streaming chat:

```bash
npm install ai @ai-sdk/react @ai-sdk/google react-markdown remark-gfm
```

These packages provide:
- `ai`: Core SDK with `streamText`, `UIMessage`, `convertToModelMessages`
- `@ai-sdk/react`: `useChat` hook for client-side state management
- `@ai-sdk/google`: Gemini provider adapter
- `react-markdown`: Safe markdown rendering component
- `remark-gfm`: GitHub Flavored Markdown plugin (tables, strikethrough)

Note: The project already has `@google/genai` for extraction - keep it separate. The Vercel AI SDK provides a unified streaming interface that handles SSE, error handling, and state management automatically.
  </action>
  <verify>
```bash
npm ls ai @ai-sdk/react @ai-sdk/google react-markdown remark-gfm
```
All five packages should be listed without errors.
  </verify>
  <done>Package.json includes ai, @ai-sdk/react, @ai-sdk/google, react-markdown, remark-gfm as dependencies</done>
</task>

<task type="auto">
  <name>Task 2: Create chat configuration constants</name>
  <files>src/lib/chat/constants.ts</files>
  <action>
Create `src/lib/chat/constants.ts` with shared chat configuration:

```typescript
// Chat model configuration
export const CHAT_MODEL = 'gemini-2.5-flash';

// System prompt for MedCatalog Assistant
export const SYSTEM_PROMPT = `You are MedCatalog Assistant, a helpful AI for orthopedic medical device procurement.

Your role:
- Help users find products in the catalog
- Answer questions about medical devices, materials, and specifications
- Provide concise, professional responses
- When discussing products, mention key specs like material composition, dimensions, and regulatory status (CE marking, MDR class)

Guidelines:
- Be concise - procurement professionals value efficiency
- Use markdown formatting for clarity (tables, lists, bold for emphasis)
- If you don't know something specific about the catalog, say so
- For product searches, you'll gain tool access in future updates

Current capabilities: General conversation about orthopedic medical devices and the catalog.`;
```

This centralizes configuration so both the route handler and future components can reference the same model and prompt.
  </action>
  <verify>
```bash
cat src/lib/chat/constants.ts
```
File exists with CHAT_MODEL and SYSTEM_PROMPT exports.
  </verify>
  <done>src/lib/chat/constants.ts exists with exported CHAT_MODEL and SYSTEM_PROMPT constants</done>
</task>

<task type="auto">
  <name>Task 3: Create streaming chat API route</name>
  <files>src/app/api/chat/route.ts</files>
  <action>
Create `src/app/api/chat/route.ts` implementing the streaming endpoint:

```typescript
import { streamText, UIMessage, convertToModelMessages } from 'ai';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { SYSTEM_PROMPT, CHAT_MODEL } from '@/lib/chat/constants';

// Initialize Google provider with existing GEMINI_API_KEY
const google = createGoogleGenerativeAI({
  apiKey: process.env.GEMINI_API_KEY,
});

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: google(CHAT_MODEL),
    system: SYSTEM_PROMPT,
    messages: await convertToModelMessages(messages),
    abortSignal: req.signal, // CRITICAL: Pass abort signal for cleanup
  });

  return result.toUIMessageStreamResponse();
}
```

Key implementation notes:
1. Uses `createGoogleGenerativeAI` with existing `GEMINI_API_KEY` env var (not `GOOGLE_GENERATIVE_AI_API_KEY`)
2. `abortSignal: req.signal` - CRITICAL for memory leak prevention. When client calls `stop()`, this terminates server-side generation
3. `toUIMessageStreamResponse()` returns proper SSE format that `useChat` hook expects
4. `convertToModelMessages` handles the UIMessage -> ModelMessage conversion

Do NOT:
- Use `StreamingTextResponse` (deprecated)
- Forget the abortSignal passthrough (causes token waste on abort)
- Create a separate Gemini client (use the SDK's provider pattern)
  </action>
  <verify>
```bash
# Verify file exists and has correct structure
cat src/app/api/chat/route.ts

# Test endpoint with curl (requires dev server)
# Start dev server in another terminal first
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello"}]}'
```

The curl should return a streaming response (multiple data: chunks with text content). If GEMINI_API_KEY is not set, expect an API key error (valid - indicates route is working but needs env var).
  </verify>
  <done>POST /api/chat accepts messages array and returns streaming SSE response from Gemini</done>
</task>

</tasks>

<verification>
Phase infrastructure verification:

1. **Dependencies installed:**
   ```bash
   npm ls ai @ai-sdk/react @ai-sdk/google react-markdown remark-gfm
   ```

2. **Files created:**
   ```bash
   ls -la src/lib/chat/constants.ts src/app/api/chat/route.ts
   ```

3. **Build check:**
   ```bash
   npm run build
   ```
   Should complete without TypeScript errors.

4. **Manual streaming test:**
   - Start dev server: `npm run dev`
   - Send POST to /api/chat with a simple message
   - Verify response streams incrementally (not all at once)
</verification>

<success_criteria>
- All 5 npm packages installed and listed in package.json
- src/lib/chat/constants.ts exports SYSTEM_PROMPT and CHAT_MODEL
- src/app/api/chat/route.ts exports POST handler
- Route handler passes req.signal to abortSignal (memory leak prevention)
- npm run build completes without errors
</success_criteria>

<output>
After completion, create `.planning/phases/08-streaming-foundation/08-01-SUMMARY.md`
</output>
